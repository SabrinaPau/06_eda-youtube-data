{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction into Exploratory Data Analysis \n",
    "## Exploring Daily Statistics for Trending YouTube Videos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/e/e1/Logo_of_YouTube_%282015-2017%29.svg\", width = 800)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**WHY EDA?**  \n",
    "Statistical analyses in order to solve tasks and confirm/reject hypotheses are part of the Data Analyst Life Cycle.  \n",
    "Initial investigations on your data are key in order to understand them - which again is  necessary for further data analysis and future predictions.  \n",
    "Within an EDA, you will perform initial investigations and aim for:  \n",
    "- discover patterns \n",
    "- spot anomalies \n",
    "- test hypotheses \n",
    "- check assumptions "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OBJECTIVE OF THIS NOTEBOOK**  \n",
    "This notebook contains an example EDA on daily statistics for trending YouTube videos.\n",
    "We will go through the cleaning and feature engineering part of an EDA as well as the hypotheses making and testing part.\n",
    "After going through this code-along you will have an idea about\n",
    "- how to approach an EDA\n",
    "- the single parts of an EDA\n",
    "- what lines of code are commonly used within an EDA\n",
    "- how to build and answer hypotheses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CONTENT OF THIS NOTEBOOK**  \n",
    "In this notebook we will have a look at daily statistics for trending YouTube videos.  \n",
    "\n",
    "The data as well as the data-descriptions are retrieved from [Kaggle](https://www.kaggle.com/datasnaek/youtube-new). The data was collected from Google's You Tube API using an opensource tool available on [Github](https://github.com/mitchelljy/Trending-YouTube-Scraper)\n",
    "There you can find the following description of this data:  \n",
    "\"YouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.\"  \n",
    "\n",
    "*What does trending mean?*  \n",
    "According to [YouTube](https://support.google.com/youtube/answer/7239739?hl=en), trending helps viewers see what’s happening on YouTube and in the world. Trending aims to surface videos that a wide range of viewers would find interesting. Some trends are predictable, like a new song from a popular artist or a new movie trailer. Others are surprising, like a viral video. Trending isn’t personalized and displays the same list of trending videos in each country to all users.  \n",
    "The list of trending videos is updated roughly every 15 minutes. With each update, videos may move up, down, or stay in the same position in the list.\n",
    "\n",
    "Data are available for different countries as seperate files, in this notebook we will use the daily statistics for Germany for the years 2017/2018.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BUSINESS CASE**  \n",
    "You work as Data Analyst for a music agency, which has a lot of prospective music stars under contract.  \n",
    "The agency instructed you to give them an overview about how likely it is, that videos of their stars become trending videos on YouTube.  \n",
    "They want to know, what number of views, likes, dislikes the most popular trending videos had in the past.    \n",
    "Do music videos become trending videos at all? Which channels might be a good choice? When is a good time to publish a video?  \n",
    "\n",
    "Questions about questions - and you should answer them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's get started"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set-up your working environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import the necessary libraries you need for your analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Floats (decimal numbers) should be displayed rounded with 2 decimal places\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "# Set style for plots\n",
    "plt.style.use('fivethirtyeight') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understand your Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As first part of an EDA it is important to find out what information is contained in the data set.  \n",
    "We start by reading in our data and get some basic information for our dataset.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read in csv file and display first 5 rows of datset\n",
    "df = pd.read_csv(\"data/DEvideos.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check which columns are included in our dataframe\n",
    "df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to really understand the data, we need to know what information the different columns contain.    \n",
    "Here are the explanations of the individual variables:\n",
    "\n",
    "\n",
    "| Column name | Description |\n",
    "| --- | ----------- |\n",
    "| video_id | Unique identifier for each uploaded video|\n",
    "| trending_date | Date on which this video was trending|\n",
    "| title | Video tite|\n",
    "| channel_title | Name of YouTube channel |\n",
    "| category_id  | Category video belongs to |\n",
    "| publish_time| Date and time, video was published |\n",
    "| tags| Tags used for this video|\n",
    "| views | Number of views |\n",
    "| likes | Number of likes |\n",
    "| dislikes | Number of dislikes |\n",
    "| comment_count | Number of comments |\n",
    "| thumbnail link | Link to reduced-size versions of pictures or videos |\n",
    "| comments_disabled | States, if comments were disabled or not |\n",
    "| ratings_disabled | States, if ratings were disabled or not |\n",
    "| video_error_or_removed | States, if there was an error or if video was removed |\n",
    "| description | Description of video content |\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's have a look at the shape of our dataset, meaning how long and wide it it.\n",
    "df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have 40840 rows and 16 columns in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We now want to check out our data-types as well as get a feeling for possible missing values\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data-types**  \n",
    "- object: We see that we have a lot of objetc data-types in our dataset. This means, we have strings (meaining text) or mixed data-types in these columns. For video_id, title, channel_title, tags, thumbnail_link and description this is true - however talking about trending_date and publish_date we need to change our data-types into a date format. We will keep that in mind and come back to this later.  \n",
    "- int64: Furthermore we have integers in category_id, views, likes, dislikes and comment_count, which makes sense, since we are expecting numerical values there.  \n",
    "- bool: The boolean type indicates, that values are either True or False in these three columns - which makes sense when looking at our column names and descriptions.   \n",
    " **Missing values**  \n",
    " Below the header *Non-Null* we see how many non-null values we have per column.  Except for description, we have 40840 non-null values in each column which is exactely the same number as we have rows.   \n",
    " Meaning: We only seem to have missing values in the description column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Hypotheses / Assumptions about Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After browsing through our data and figuring out, what data we have in front of us, we now want to build some hypotheses (in relation to our expectations about what is contained in the data.  \n",
    "Generally we assume the data is complete (has no missing values) and is correct (has no obvious logical problems that defy our understanding of the content).  \n",
    "We can test these assumptions and confirm our data is valid before moving on with our analysis.  \n",
    "\n",
    "You will realize, that there is some work to do before starting with our actual analysis."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explore and clean your data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to test our hypotheses, we will now explore our data in more detail.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Drop columns**  \n",
    "Since we are not interested in all the columns, we can drop some of these to make our exploration easier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop columns we don't need\n",
    "df.drop([\"tags\",\"thumbnail_link\", \"comments_disabled\", \"ratings_disabled\", \"video_error_or_removed\", \"description\"], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Deal with duplicate rows**  \n",
    "Let's check if we have the same videos (with the same video_id) serveral times in our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check for duplicate rows in video_id column\n",
    "df[\"video_id\"].duplicated().value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we have data on 40840 trending videos to analyze. On YouTube, the same video might appear on the trending list for many days. This means that the 40840 videos are not unique videos. In fact, among the 40840 videos, we have 29627 unique videos.  \n",
    "These duplicate rows exist, since some of the videos stayed on the trending list for more than one day."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As stated in [this analysis](https://ammar-alyousfi.com/2020/youtube-trending-videos-analysis-2019-us), we will apply the following analysis on all of our 40840 trending videos and not only on the unique trending videos. This will give videos which were trending for several days more weight, which is in our interest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transform data types**  \n",
    "As stated above, we need to transform trending_date and publish_time into a datetime format.  \n",
    "Fortunately, there is a built-in function in pandas, which will convert an argument to datetime."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert trending_date and publish_time into datetime and extract date part from publish_time\n",
    "# format specifies the present form of our argument we pass into the function\n",
    "df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')\n",
    "df['publish_time'] = pd.to_datetime(df['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "df['publish_date'] = pd.to_datetime(df['publish_time'].dt.date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Build new columns/ Feature engineering**  \n",
    "Since we want to know, when most trending videos get published, we need to create new columns which contain information about the day of the week as well as the time of the day.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# extract year, month, dayofweek and hour information out of column publish_time and build new column for each\n",
    "df[\"publish_year\"]=df[\"publish_time\"].dt.year\n",
    "df[\"publish_month\"]=df[\"publish_time\"].dt.month\n",
    "df[\"publish_weekday\"]=df[\"publish_time\"].dt.dayofweek\n",
    "df[\"publish_hour\"]=df[\"publish_time\"].dt.hour"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note*:\n",
    "Day of week: Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also to this for the trending date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# extract year, month, dayofweek out of column trending_date and build new column for each\n",
    "df[\"trending_year\"]=df[\"trending_date\"].dt.year\n",
    "df[\"trending_month\"]=df[\"trending_date\"].dt.month\n",
    "df[\"trending_weekday\"]=df[\"trending_date\"].dt.dayofweek"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to know, what categories have most of the trending videos - however so far, we only have an category ID and don't know what's behind this ID.  \n",
    "Fortunately we can decode the IDs and transform them into actual category names."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create dictionary called category_names with category_id as key and the actual category name as value\n",
    "# we get these values from the Google API or here: https://gist.github.com/dgp/1b24bf2961521bd75d6c\n",
    "category_names = {\n",
    "    1:'Film and Animation',\n",
    "    2:'Autos & Vehicles',\n",
    "    10:'Music',\n",
    "    15:'Pets and Animals',\n",
    "    17:'Sports',\n",
    "    18:'Short Movies',\n",
    "    19:'Travel and Events',\n",
    "    20:'Gaming',\n",
    "    21:'Videoblogging',\n",
    "    22:'People and Blogs',\n",
    "    23:'Comedy',\n",
    "    24:'Entertainment',\n",
    "    25:'News and Politics',\n",
    "    26:'How to and Style',\n",
    "    27:'Education',\n",
    "    28:'Science and Technology',\n",
    "    29:'Non Profits and Activism',\n",
    "    30:'Movies',\n",
    "    31:'Anime/Animation', \n",
    "    32:'Action/Adventure',\n",
    "    33:'Classics',\n",
    "    34:'Comedy', \n",
    "    35:'Documentary',\n",
    "    36:'Drama',\n",
    "    37:'Family',\n",
    "    38:'Foreign',\n",
    "    39:'Horror',\n",
    "    40:'Sci-Fi/Fantasy',\n",
    "    41:'Thriller',\n",
    "    42:'Shorts',\n",
    "    43:'Shows',\n",
    "    44:'Trailers'\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create new column called category_name wich contains the name of the category, the video belongs to\n",
    "# the map() function is used to map values of Series according to input correspondence (https://www.w3resource.com/pandas/series/series-map.php)\n",
    "df['category_name'] = df['category_id'].map(category_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df[[\"title\", \"category_id\", \"category_name\"]].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Days between publish and trending\n",
    "In our hypotheses we stated that we also want to have a look at the time-interval between video-publication-date and the trending_date.\n",
    "Thus, we need to create a new column including the time interval in days."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create new column called trending_days_difference\n",
    "df['trending_days_difference']=(df[\"trending_date\"]-df[\"publish_date\"]).dt.days"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check result\n",
    "df[[\"trending_date\", \"publish_date\", \"trending_days_difference\"]].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Missing values**  \n",
    "We have already seen above, that we only have missing values in the description column, which we have dropped.  \n",
    "Let's confirm this again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# displays sum of missing values per column\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no other missing values we have to deal with."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Descriptive Statistics**  \n",
    "Let's have a look at our numerical variables and their descriptive statistics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the describe() function gives you a good overview\n",
    "df[[\"views\", \"likes\", \"dislikes\", \"comment_count\", \"trending_days_difference\"]].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Short explanation of the reported measures:\n",
    "- count: Indication of how many values are present in the columns (NaNs/missing values are not counted).\n",
    "- mean: average value of the data\n",
    "- std: standard deviation of the data\n",
    "- min: the smallest value in the data set\n",
    "- 25%: 25 % of the data are below this value\n",
    "- 50%: 50% of the data are below this value. This value is called the median.\n",
    "- 75%: 75% of the data are below this value\n",
    "- max: the largest expression in the data set\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task\n",
    "#### What can you take away from this table?\n",
    "\n",
    "Please take some time to consider if the values make sense and make notes where things meet your expectations or what sems out of place.  \n",
    "\n",
    "1.  \n",
    "2.  \n",
    "3.  \n",
    "...  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**From the table above we note that**\n",
    "- a trending video has on average 603.455 views, 21.875 likes, 1.397 dislikes and 2.785 comments\n",
    "- on average the time between video is published and video is trending is 1,85 days\n",
    "- the median value for the number of views is 119.277 which means that half of the trending videos have views that are less than that number, and the other half have views larger than that number. The same is true for the median of likes, dislikes, comment_count and trending_days_difference\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Outliers**  \n",
    "We see in our descriptive statistics, that the minimim and maximum values in our numerical columns are far apart from each other. Let's have a look at these \"extreme\" values by looking at boxplots.  \n",
    "*Note:* The following code creates several subplots, meaning that all our numerical variables are plotted next to each other. This can be done by specifying the axes you want to plot each variable on (ax[0][0] means: first row, first column)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(20,10)) # create subplots on 2 rows and 3 columns\n",
    "plt.suptitle('Distribution of numeric columns', fontsize=20)\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "ax[0][0].boxplot(x = df['views']) # creates boxplot for number of views\n",
    "ax[0][0].set_xticklabels(labels=[\"Views\"]) # sets the label for the ticks on the x-axis\n",
    "ax[0][0].set_ylim(0,2000000) # limits the y-axis values from 0 to 2000000 \n",
    "ax[0][0].set_title(\"Distribution of views count\", fontsize = 15); # sets title for subplot\n",
    "\n",
    "ax[0][1].boxplot(x = df['likes'])\n",
    "ax[0][1].set_xticklabels(labels=[\"Likes\"])\n",
    "ax[0][1].set_ylim(0,50000)\n",
    "ax[0][1].set_title(\"Distribution of likes count\", fontsize = 15);\n",
    "\n",
    "ax[0][2].boxplot(x = df['dislikes'])\n",
    "ax[0][2].set_xticklabels(labels=[\"Dislikes\"])\n",
    "ax[0][2].set_ylim(0,6000)\n",
    "ax[0][2].set_title(\"Distribution of dislikes count\", fontsize = 15);\n",
    "\n",
    "ax[1][0].boxplot(x = df['comment_count'])\n",
    "ax[1][0].set_xticklabels(labels=[\"Comment count\"])\n",
    "ax[1][0].set_ylim(0,20000)\n",
    "ax[1][0].set_title(\"Distribution of comment count\", fontsize = 15);\n",
    "                              \n",
    "ax[1][1].boxplot(x = df['trending_days_difference'])\n",
    "ax[1][1].set_xticklabels(labels=[\"Trending days difference\"])\n",
    "ax[1][1].set_ylim(-1,10)\n",
    "ax[1][1].set_title(\"Distribution of difference in trending time\", fontsize = 15);\n",
    "\n",
    "fig.delaxes(ax[1][2]);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we can see is that most of the values lie in the lower part of our distribution (50% of our data lies below the red line which represents the median).  \n",
    "But there are also a couple of videos with a very high number of views, likes, dislikes and comments. Since we can expect some videos to become much more popular than others, these extreme values can be expected and should not be removed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task\n",
    "#### How would you describe these distribution? \n",
    "Please take some time to think about this question and take some notes here.  \n",
    "\n",
    "1.  \n",
    "2.  \n",
    "3.  ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the distribution with the help of histograms. This might take some time to run, please be patient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(20,10)) # create subplots on 2 rows and 3 columns\n",
    "plt.suptitle('Distribution of numeric columns', fontsize=20)\n",
    "fig.tight_layout() # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "ax[0][0].hist(x=df['views'], bins=10000)  # creates histogram for number of views\n",
    "ax[0][0].set_ylabel(\"No of videos\", fontsize = 10) # sets the label y-axis\n",
    "ax[0][0].set_xlim(0,1000000) # limits the x-axis values from 0 to 1000000 \n",
    "ax[0][0].set_title(\"Distribution of views count\", fontsize = 15);  # sets title for subplot\n",
    "\n",
    "ax[0][1].hist(x= df['likes'], bins=5000)\n",
    "ax[0][1].set_ylabel(\"No of videos\", fontsize = 10)\n",
    "ax[0][1].set_xlim(0,50000)\n",
    "ax[0][1].set_title(\"Distribution of likes count\", fontsize = 15);\n",
    "\n",
    "ax[0][2].hist(x=df['dislikes'], bins=5000)\n",
    "ax[0][2].set_ylabel(\"No of videos\", fontsize = 10)\n",
    "ax[0][2].set_xlim(0,6000)\n",
    "ax[0][2].set_title(\"Distribution of dislikes count\", fontsize = 15);\n",
    "\n",
    "ax[1][0].hist(x=df['comment_count'], bins=1000)\n",
    "ax[1][0].set_ylabel(\"No of videos\", fontsize = 10)\n",
    "ax[1][0].set_xlim(0,20000)\n",
    "ax[1][0].set_title(\"Distribution of comment count\", fontsize = 15);\n",
    "                              \n",
    "ax[1][1].hist(x=df['trending_days_difference'], bins=1000)\n",
    "ax[1][1].set_ylabel(\"No of videos\", fontsize = 10)\n",
    "ax[1][1].set_xlim(0,10)\n",
    "ax[1][1].set_title(\"Distribution of difference in trending time\", fontsize = 15);\n",
    "\n",
    "fig.delaxes(ax[1][2]);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see, that we have right skewed distributions, meaning that the mean, median, and mode are all different from each other. In this case, the mode is the highest point of the histogram, whereas the median and mean fall to the right of it (or, visually, the right of the peak). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**We have made it through the data cleaning process!**  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Addressing the Business Case: Hypotheses Driven Approach\n",
    "\n",
    "We have a dataset that we understand and know to contain valid (or at least plausible) data. We can go ahead and try to create some knowledge and insights to share with our colleagues. We do this by stating our initial ideas as hypotheses that will will then test with the data to see if we they can be confirmed or rejected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step after browsing through our data confirming it is clean and logical, we now want to build some hypotheses in relation to our business case.  \n",
    "The next task will be to look at the data columns in order to develop some hypotheses.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task \n",
    "Write down your own hypotheses about what we could learn from the dataset.   \n",
    "The hypotheses should be a statement about some insights or relationship you might find in the data. They can be written as a positive statement that you can prove or disprove.\n",
    "\n",
    "1.  \n",
    "2.  \n",
    "3.  \n",
    "\n",
    "...  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our Hypotheses\n",
    "Since we all need to follow the same hypotheses in this code-along, we use the following hypotheses for exploration during the next steps:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Video-Categories differ in amount of trending videos, views, likes and dislikes\n",
    "2. For certain categories it takes longer to become a trending video after publication than for other categories\n",
    "3. Some YouTube channels are more successful than others\n",
    "4. The amount of video publications differ per day and hour\n",
    "5. Correlations exist between views, likes, dislikes and comments\n",
    "6. The the length of time-interval between the date a video gets published and a video becomes trending also is correlated to number of views, likes, dislikes and comments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following we will use the plotting library seaborn (abbreviated by sns). Seaborn is built on top of Matplotlib and extends its options. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Video-Categories differ in amount of trending videos, views, likes and dislikes**  \n",
    "**2. For certain categories it takes longer to become a trending video after publication than for other categories**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot number of videos per category\n",
    "plt.figure(figsize=(15,8)) # specify figure size\n",
    "f1 = sns.countplot(x=df['category_name']) # create countplot\n",
    "f1.tick_params(axis='x', rotation=90) # get x-ticks and rotate them\n",
    "f1.set(xlabel=None) # set no label for x-axis \n",
    "f1.set_ylabel(\"No of videos\", fontsize = 10); # set label for y-axis\n",
    "f1.set_title('Number of videos per category', fontsize=20); # set title"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see, that most of the trending videos belong to the entertainment category. Music is more in the midfield when it comes to the amount of trending videos. The Categories will be represented by the same color in each of the following charts so note the categories we are primarily interested in."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot views, likes, dislikes and comments per category\n",
    "fig, ax = plt.subplots(2,3, figsize=(20,15)) # create subplots on 2 rows and 2 columns\n",
    "plt.suptitle('Likes, dislikes, views and comments per category', fontsize=20) \n",
    "fig.tight_layout()   # tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "plt.subplots_adjust(hspace = .6, wspace = .2, top = .9) # adjusts the space between the single subplots\n",
    "\n",
    "f1 = sns.barplot(x=df[\"category_name\"], y = df['views'], ax=ax[0,0]) # creates barchart for number of views\n",
    "f1.tick_params(axis='x', labelrotation=90) # sets the label y-axis\n",
    "f1.set(xlabel=None) # sets the x-axis label\n",
    "f1.set_ylabel(\"Number of views\", fontsize = 10) # limits the x-axis values from 0 to 1000000\n",
    "f1.set_title(\"Views per category\", fontsize = 15);  # sets title for subplot\n",
    "\n",
    "f2= sns.barplot(x=df[\"category_name\"], y = df['likes'], ax=ax[0,1])\n",
    "f2.tick_params(axis='x', labelrotation=90)\n",
    "f2.set(xlabel=None)\n",
    "f2.set_ylabel(\"Number of likes\", fontsize = 10)\n",
    "f2.set_title(\"Likes per category\", fontsize = 15);\n",
    "\n",
    "f3 = sns.barplot(x=df[\"category_name\"], y = df['dislikes'], ax=ax[0,2])\n",
    "f3.tick_params(axis='x', labelrotation=90)\n",
    "f3.set(xlabel=None)\n",
    "f3.set_ylabel(\"Number of dislikes\", fontsize = 10)\n",
    "f3.set_title(\"Dislikes per category\", fontsize = 15);\n",
    "\n",
    "f4 = sns.barplot(x=df[\"category_name\"], y = df['comment_count'], ax=ax[1,0])\n",
    "f4.tick_params(axis='x', labelrotation=90)\n",
    "f4.set(xlabel=None)\n",
    "f4.set_ylabel(\"Number of comments\", fontsize = 10)\n",
    "f4.set_title(\"Comments per category\", fontsize = 15);\n",
    "\n",
    "f5 = sns.barplot(x=df[\"category_name\"], y = df['trending_days_difference'], ax=ax[1,1])\n",
    "f5.tick_params(axis='x', labelrotation=90)\n",
    "f5.set(xlabel=None)\n",
    "f5.set_ylabel(\"Time period in days\", fontsize = 10)\n",
    "f5.set_title(\"Time period for trending per category\", fontsize = 15);\n",
    "\n",
    "fig.delaxes(ax[1][2]);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow, music seems to be a promising category -  The videos belonging to this category have the most views, the most likes and the second most comments.  \n",
    "It is surprising though that the category with the most trending videos (Entertainment) does not have the highest number of likes, views and comments.  \n",
    "\n",
    "The time period for music videos to get trending is in comparison quite high - we should tell our agency to stay patient."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.Some YouTube channels are more successful than others**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to answer this hypothesis, we will have a look the YouTube channels which have published the most trending videos in the music category."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top5 channels across all categories\n",
    "df['channel_title'].value_counts().head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Top5 channels for category music\n",
    "df.query(\"category_name == 'Music'\")['channel_title'].value_counts().head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "385idéal seems to publish most trending videos. We should do some research on this channel and recommend it to our colleagues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. The amount of video publications differ per day and hour**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(20,10))\n",
    "plt.suptitle('Video publication time', fontsize=20)\n",
    "fig.tight_layout(h_pad=8) \n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .9)\n",
    "\n",
    "f1 = sns.countplot(x=df['publish_weekday'], ax=ax[0])\n",
    "f1.set_ylabel(\"No of published videos\", fontsize = 10)\n",
    "f1.set_xlabel(\"Weekday of publication\", fontsize = 10)\n",
    "f1.set_xticks([0,1,2,3,4,5,6])\n",
    "f1.set_xticklabels(labels=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "f1.set_title(\"Number of published videos per weekday\", fontsize = 15);\n",
    "\n",
    "f2 = sns.countplot(x=df['publish_hour'], ax=ax[1])\n",
    "f2.set_ylabel(\"No of published videos\", fontsize = 10)\n",
    "f2.set_xlabel(\"Hour of publication\", fontsize = 10)\n",
    "f2.set_title(\"Number of published videos per hour\", fontsize = 15);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On fridays most trending videos got published - looking at the exact time of the day, 5pm is the time, when most trending videos got published.  \n",
    "Let's do the same analysis only for the music category. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_music = df.query(\"category_name == 'Music'\")\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(20,10))\n",
    "plt.suptitle('Video publication time for Music videos', fontsize=20)\n",
    "fig.tight_layout(h_pad=8) \n",
    "plt.subplots_adjust(hspace = .5, wspace = .2, top = .9)\n",
    "\n",
    "f1 = sns.countplot(x=df_music['publish_weekday'], ax=ax[0])\n",
    "f1.set_ylabel(\"No of published videos\", fontsize = 10)\n",
    "f1.set_xlabel(\"Weekday of publication\", fontsize = 10)\n",
    "f1.set_xticks([0,1,2,3,4,5,6])\n",
    "f1.set_xticklabels(labels=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "f1.set_title(\"Number of published videos per weekday\", fontsize = 15);\n",
    "\n",
    "f2 = sns.countplot(x=df_music['publish_hour'], ax=ax[1])\n",
    "f2.set_ylabel(\"No of published videos\", fontsize = 10)\n",
    "f2.set_xlabel(\"Hour of publication\", fontsize = 10)\n",
    "f2.set_title(\"Number of published videos per hour\", fontsize = 15);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Friday seems to be THE day to publish the videos of our stars! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check for correlations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Correlations exist between views, likes, dislikes and comments**  \n",
    "**6. The the length of time-interval between the date a video gets published and a video becomes trending also is correlated to number of views, likes, dislikes and comments**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# correlation matrix between these variables\n",
    "corr_columns=[\"views\", \"likes\", \"dislikes\", \"comment_count\", \"trending_days_difference\"]\n",
    "corr_mtrx=df[corr_columns].corr()\n",
    "corr_mtrx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# draw the heatmap \n",
    "plt.figure(figsize = (14,12))\n",
    "ax = sns.heatmap(corr_mtrx, linewidths=.5, annot=True, cmap='coolwarm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So there are notable positive relationship between\n",
    "- Views and: Likes and Comments \n",
    "- Likes and: Comments\n",
    "- Dislikes and Comments\n",
    "\n",
    "There are no relationships between these variables and the time period between publish date and trending date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task\n",
    "\n",
    "1. Summarize the major issues and actions that needed to be addressed in cleaning and augmenting part. Could the decisions we took about cleaning affect the end understanding of the business case?   \n",
    "2. Summarize the findings of our EDA here. What will you tell your colleagues from the agency?   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nf_base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "0e4771abb73651cc71498e03f3559c7e0f15f38d5124065b3832974a7bbffea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}